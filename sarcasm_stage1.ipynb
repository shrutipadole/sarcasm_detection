{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K7rCXwkMu2bx",
    "outputId": "cb99ab6f-d1be-475c-da96-497066e3d2cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /opt/conda/lib/python3.7/site-packages (3.2)\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0-cp37-cp37m-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (24.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.0 MB 8.1 MB/s eta 0:00:011   |████████                        | 6.0 MB 2.1 MB/s eta 0:00:09\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.7.3)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.21.5)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.2.0 smart-open-6.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install wget\n",
    "!pip install gensim\n",
    "import wget\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "import string\n",
    "LANGUAGE = string.ascii_lowercase + string.punctuation + ' '\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, RegexpStemmer \n",
    "# ps = PorterStemmer()\n",
    "# # ps = SnowballStemmer(language='english')\n",
    "# # ps = LancasterStemmer()\n",
    "# # ps = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eE-ZDppXtJ7X"
   },
   "outputs": [],
   "source": [
    "COMMON_ABBREV = {\n",
    "    \"411\":\"information\",\n",
    "    \"af\":\"as fuck\",\n",
    "    \"afaik\":\"as far as i know\",\n",
    "    \"ama\":\"ask me anything\",\n",
    "    \"asl\":\"age / sex / location\",\n",
    "    \"b4\":\"before\",\n",
    "    \"bae\":\"before anyone else\",\n",
    "    \"b/c\":\"because\",\n",
    "    \"bc\":\"because\",\n",
    "    \"bff\":\"best friends forever\",\n",
    "    \"brb\":\"be right back\",\n",
    "    \"btaim\":\"be that as it may\",\n",
    "    \"bts\":\"behind the scenes\",\n",
    "    \"btw\":\"by the way\",\n",
    "    \"dae\":\"does anyone know?\",\n",
    "    \"dftba\":\"don’t forget to be awesome\",\n",
    "    \"dyk\":\"did you know\",\n",
    "    \"eli5\":\"explain like i am 5 ( years old )\",\n",
    "    \"f2f\":\"face to face\",\n",
    "    \"fbf\":\"flashback friday\",\n",
    "    \"ffs\":\"for fuck’s sake\",\n",
    "    \"fml\":\"fuck my life\",\n",
    "    \"fomo\":\"fear of missing out\",\n",
    "    \"ftfy\":\"fixed that for you\",\n",
    "    \"ftw\":\"for the win\",\n",
    "    \"futab\":\"feet up, take a break\",\n",
    "    \"fwiw\":\"for what it is worth\",\n",
    "    \"fyi\":\"for your information\",\n",
    "    \"gg\":\"good game\",\n",
    "    \"gr8\":\"great\",\n",
    "    \"gtg\":\"got to go\",\n",
    "    \"gtr\":\"got to run\",\n",
    "    \"h/t\":\"hat tip\",\n",
    "    \"hbd\":\"happy birthday\",\n",
    "    \"hth\":\"here to help / happy to help\",\n",
    "    \"hmb\":\"hit me back\",\n",
    "    \"hmu\":\"hit me up\",\n",
    "    \"ianad\":\"i am not a doctor\",\n",
    "    \"ianal\":\"i am not a lawyer\",\n",
    "    \"icymi\":\"in case you missed it\",\n",
    "    \"idc\":\"i do not care\",\n",
    "    \"idk\":\"i do not know\",\n",
    "    \"ikr\":\"i know, right?\",\n",
    "    \"ily\":\"i love you\",\n",
    "    \"imho\":\"in my humble opinion\",\n",
    "    \"imo\":\"in my opinion\",\n",
    "    \"imy\":\"i miss you\",\n",
    "    \"irl\":\"in real life\",\n",
    "    \"iso\":\"in search of\",\n",
    "    \"jk\":\"just kidding\",\n",
    "    \"jtm\":\"just the messenger\",\n",
    "    \"l8\":\"late\",\n",
    "    \"lmao\":\"laughing my ass off\",\n",
    "    \"lmk\":\"let me know\",\n",
    "    \"lol\":\"laughing out loud\",\n",
    "    \"mtfbwy\":\"may the force be with you\",\n",
    "    \"myob\":\"mind your own business\",\n",
    "    \"nbd\":\"no big deal\",\n",
    "    \"nm\":\"not much\",\n",
    "    \"nsfw\":\"not safe for work\",\n",
    "    \"nvm\":\"nevermind\",\n",
    "    \"nyt\":\"name your trade\",\n",
    "    \"obv\":\"obviously\",\n",
    "    \"oh\":\"overheard\",\n",
    "    \"omg\":\"oh my god\",\n",
    "    \"omw\":\"on my way\",\n",
    "    \"orly\":\"oh really?\",\n",
    "    \"pls\":\"please\",\n",
    "    \"ppl\":\"people\",\n",
    "    \"potd\":\"photo of the day\",\n",
    "    \"psa\":\"public service announcement\",\n",
    "    \"qotd\":\"quote of the day\",\n",
    "    \"rn\":\"right now\",\n",
    "    \"rofl\":\"rolling on the floor laughing\",\n",
    "    \"srsly\":\"seriously\",\n",
    "    \"smh\":\"shaking my head\",\n",
    "    \"tbh\":\"to be honest\",\n",
    "    \"tbt\":\"throwback thursday\",\n",
    "    \"tfw\":\"that feeling when / the face when\",\n",
    "    \"tgif\":\"thank god it’s friday\",\n",
    "    \"thx\":\"thanks\",\n",
    "    \"til\":\"today i learned\",\n",
    "    \"tl;dr\":\"too long; did not read\",\n",
    "    \"tmi\":\"too much information\",\n",
    "    \"ty\":\"thank you\",\n",
    "    \"wbu\":\"what about you?\",\n",
    "    \"wbw\":\"wayback wednesday\",\n",
    "    \"wfh\":\"working from home\",\n",
    "    \"wtf\":\"what the fuck\",\n",
    "    \"wyd\":\"what are you doing?\",\n",
    "    \"yolo\":\"you only live once\",\n",
    "    \"ysk\":\"you should know\",\n",
    "    \"yw\":\"you are welcome\"\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_8hSW1Frttla"
   },
   "outputs": [],
   "source": [
    "def download_file(url):\n",
    "    \"\"\"\n",
    "    input : url to zip files of amazon customer reviews\n",
    "            Eg: \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Wireless_v1_00.tsv.gz\"\n",
    "    output : path to downloaded file\n",
    "             Eg: '../data/dataset/sample_us.tsv'\n",
    "    \"\"\"\n",
    "    # import pdb;pdb.set_trace()\n",
    "    # path = '../data/dataset'\n",
    "    # zip_path = path + '/' + url.split('/')[-1]\n",
    "    zip_path = url.split('/')[-1]\n",
    "    unzip_path = zip_path.replace(\".gz\",'')\n",
    "\n",
    "    if not os.path.exists(zip_path): #zip file not present \n",
    "        if not os.path.exists(unzip_path):\n",
    "            print(\"Downloading ZIP file becasue File does not exist\")\n",
    "            filename = wget.download(url)\n",
    "            print(filename)\n",
    "            print(\"Dowload Complete\\n\\nInitiating Decompression\")\n",
    "            tsv_path = unzip(filename)\n",
    "            print(tsv_path)\n",
    "            print(\"Decompression Complete\")\n",
    "            # return filename\n",
    "        else:\n",
    "            print(\"Zip File Already Downloaded and Decompressed\")\n",
    "            tsv_path = unzip_path\n",
    "    else: # zip file present already\n",
    "        if os.path.exists(unzip_path):\n",
    "            print(\"Zip File Already Downloaded and Decompressed\")\n",
    "            tsv_path = unzip_path\n",
    "        else:\n",
    "            print(\"Zip File Already Downloaded.\\n\\nInitiating Decompression\")\n",
    "            tsv_path = unzip(zip_path)\n",
    "            print(tsv_path)\n",
    "            print(\"Decompression Complete\")\n",
    "\n",
    "    return tsv_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xu9XgyDwuzla"
   },
   "outputs": [],
   "source": [
    "def delete(filename):\n",
    "    \"\"\"\n",
    "    input : filepath to be deleted\n",
    "            Eg: \"\"\n",
    "    output : None\n",
    "    \"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "    else:\n",
    "        print(\"The file does not exist:\\t\" + filename)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mbwCQpW3u0l3"
   },
   "outputs": [],
   "source": [
    "def unzip(infile):\n",
    "    \"\"\"\n",
    "    input : just the filename to be unzipped. \n",
    "            Accpeted file type : .gz\n",
    "            Eg: \".gz\"\n",
    "    output : name of unzipped file\n",
    "             Eg:\n",
    "    \"\"\"\n",
    "    # infile = '../data/dataset/' + filename.split('/')[-1]\n",
    "    if infile.endswith('.gz'):\n",
    "        tofile = infile.replace('.gz','')\n",
    "\n",
    "        with open(infile, 'rb') as inf, open(tofile, 'w', encoding='utf8') as tof:\n",
    "            decom_str = gzip.decompress(inf.read()).decode('utf-8')\n",
    "            tof.write(decom_str)\n",
    "        delete(infile)\n",
    "        return tofile\n",
    "    else:\n",
    "        return infile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "OGAG_vb_u5D3"
   },
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    \"\"\"\n",
    "    input : path of tsv file to be loaded. \n",
    "            Eg: \"\"\n",
    "    output : dataframe with tsv dataset loaded\n",
    "    \"\"\"\n",
    "    # import pdb;pdb.set_trace()\n",
    "    ## based on pandas version , one of the two syntaxes below should run\n",
    "    print(filename)\n",
    "    df = pd.read_csv(filename, sep='\\t', index_col = False, on_bad_lines='skip')\n",
    "    # df = pd.read_csv(filename, sep='\\t', index_col = False, error_bad_lines='skip')\n",
    "    print(df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nSnwQWpPu8Vc"
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(sent):\n",
    "    #converting all tokens to lowercase\n",
    "    sent = str(sent).lower()\n",
    "    #replace_acronyms\n",
    "    sent = \" \".join([COMMON_ABBREV[each] if each in COMMON_ABBREV else each for each in sent.split()])\n",
    "    #replacing website links\n",
    "    sent = re.sub(r\"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)\", \"\", sent)\n",
    "    #removing stopwords\n",
    "    sent = remove_stopwords(sent)\n",
    "    #stemming\n",
    "    # sent = \" \".join([ps.stem(word) for word in word_tokenize(sent)])\n",
    "    #lemmatizing\n",
    "    sent = \" \".join([LEMMATIZER.lemmatize(word) for word in word_tokenize(sent)])\n",
    "    #to filter out unicode or foreign characters\n",
    "    sent = \"\".join([char for char in sent if char in LANGUAGE])\n",
    "    #removing 's occurences\n",
    "    sent = sent.replace(\"'s\", \"\")\n",
    "    #replacing words ending in \"n't\" with \"word not\"\n",
    "    sent = sent.replace(\"wont\", \"will not\").replace(\"won't\", \"will not\").replace(\"cant\", \"can not\")\n",
    "    sent = sent.replace(\"can't\", \"can not\")\n",
    "    sent = re.sub(r\"([a-zA-z])(n't)\", r\"\\1 not\", sent)\n",
    "    sent = re.sub(r\"(would|could|is|should)(nt)\", r\"\\1 not\", sent)\n",
    "    #remove all HTML tags\n",
    "    sent = re.sub(r\"<.*?>\", \" \", sent)\n",
    "    #replacing multiple punctuations with single occurrence\n",
    "    sent = re.sub(r\"\\.+(\\s?\\.+)*\", \".\", sent)\n",
    "    sent = re.sub(r\"\\!+\", \"!\", sent)\n",
    "    sent = re.sub(r\"=+\", \"=\", sent)\n",
    "    sent = re.sub(r\"_+\", \"_\", sent)\n",
    "    #introducing space after punctuation if not already there\n",
    "    sent = re.sub(r\"([,\\.\\\"'\\?\\-:\\(\\)\\\\\\/=\\*\\!_&#])(?!\\s)\", r\"\\1 \", sent)\n",
    "    #introducing space before punctuation if not already there\n",
    "    sent = re.sub(r\"(?<!\\s)([,\\.\\\"'\\?\\-:\\(\\)\\\\\\/=\\!\\*_])\", r\" \\1\", sent)\n",
    "    #replacing words with more than 2 consecutive same character with two same characters\n",
    "    sent = re.sub(r\"([a-z])\\1{1,}\", r\"\\1\\1\", sent)\n",
    "    #replacing multiple spaces with single space\n",
    "    sent = re.sub(r\"\\s+\", r\" \", sent)\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "p73xalMlwH2A"
   },
   "outputs": [],
   "source": [
    "def main(sample):\n",
    "    \"\"\"\n",
    "    input : url to zip files of amazon customer reviews\n",
    "            Eg: \"\"\n",
    "    output : filename\n",
    "             Eg:\n",
    "    \"\"\"\n",
    "    # download_file = 'https://s3.amazonaws.com/amazon-reviews-pds/tsv/sample_us.tsv'\n",
    "    # download_file = 'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Wireless_v1_00.tsv.gz'\n",
    "    # import pdb;pdb.set_trace()\n",
    "    if sample:\n",
    "        data = ['https://s3.amazonaws.com/amazon-reviews-pds/tsv/sample_us.tsv']\n",
    "    else: \n",
    "        data = [\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Wireless_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Watches_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Video_Games_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Video_DVD_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Video_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Toys_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Tools_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Sports_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Software_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Shoes_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Pet_Products_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Personal_Care_Appliances_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_PC_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Outdoors_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Office_Products_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Musical_Instruments_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Music_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Mobile_Electronics_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Mobile_Apps_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Major_Appliances_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Luggage_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Lawn_and_Garden_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Kitchen_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Jewelry_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Home_Improvement_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Home_Entertainment_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Home_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Health_Personal_Care_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Grocery_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Gift_Card_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Furniture_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Electronics_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Video_Download_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Software_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Music_Purchase_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Ebook_Purchase_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Camera_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Books_v1_02.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Books_v1_01.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Books_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Baby_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Automotive_v1_00.tsv.gz',\n",
    "            'https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Apparel_v1_00.tsv.gz'\n",
    "            ]\n",
    "    for url in data:\n",
    "        print(\"URL >>>>   \" + url)\n",
    "        tsv_path = download_file(url)\n",
    "        print(\"Loading dataset from >>> \", tsv_path)\n",
    "        df = load_dataset(tsv_path)\n",
    "        #removing rows with null in the following two columns\n",
    "        print(\"BEFORE REMOVING NA IN review_body >>>>>\")\n",
    "        print(df.shape)\n",
    "        df = df[df['review_body'].notna()]\n",
    "        print(\"AFTER REMOVING NA IN review_body >>>>>\")\n",
    "        print(df.shape)\n",
    "        df = df[df['review_headline'].notna()]\n",
    "        print(\"AFTER REMOVING NA IN review_headline >>>>>\")\n",
    "        print(df.shape)\n",
    "        tofile = tsv_path.replace('.tsv','_trim.tsv')\n",
    "        df.to_csv(tofile, sep = '\\t', index=False)\n",
    "        print('created file >>  \\t',tofile)\n",
    "\n",
    "        #applying cleaning procedures to review_headline and review_body\n",
    "        df['clean_review_headline'] = df.apply(lambda df: preprocess(df['review_headline']), axis=1)\n",
    "        df['clean_review_body'] = df.apply(lambda df: preprocess(df['review_body']), axis=1)\n",
    "\n",
    "        #Converting 5 star ratings to binary representation. \n",
    "        #Rating >=3 is positive i.e. 1\n",
    "        #Rating <3 is negative i.e. 0\n",
    "\n",
    "        df[\"Sentiment\"] = df[\"star_rating\"].apply(lambda score: \"negative\" if int(score) < 3 else \"positive\")\n",
    "        df['Sentiment'] = df['Sentiment'].map({'positive':1, 'negative':0})\n",
    "\n",
    "        tofile = tsv_path.replace('.tsv', '_clean.tsv')\n",
    "        df.to_csv(tofile, sep = '\\t', index=False)\n",
    "        print('created file >>  \\t',tofile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OgGzyzzh0t-"
   },
   "source": [
    "# PRIMARY CELL TO TEST ABOVE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5FQeItwawPhp",
    "outputId": "fea298e6-38f6-4b8c-9a58-1a496f2be548"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with real data\n",
      "URL >>>>   https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Wireless_v1_00.tsv.gz\n",
      "Downloading ZIP file becasue File does not exist\n",
      "100% [....................................................................] 1704713674 / 1704713674amazon_reviews_us_Wireless_v1_00.tsv.gz\n",
      "Dowload Complete\n",
      "\n",
      "Initiating Decompression\n"
     ]
    }
   ],
   "source": [
    "# set choice to 2 to run with real data\n",
    "choice = 2\n",
    "# print(args)\n",
    "sample = False\n",
    "if  choice == 1:\n",
    "    sample = True\n",
    "    print(\"Running with sample data\")\n",
    "    main(sample)\n",
    "elif choice == 2:\n",
    "    print(\"Running with real data\")\n",
    "    main(sample)\n",
    "else:\n",
    "    print('Choice not supported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXwpY1hodY37"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_us_clean.tsv', sep='\\t', index_col = False, on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "FfMWfgqxdYo9",
    "outputId": "210c83e7-7555-4975-8ddb-aeb0c7dceb46"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
